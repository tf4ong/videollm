{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wr_score_util as wu \n",
    "import datetime\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from google.genai import types\n",
    "from google import genai\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from time import sleep\n",
    "import imageio.v3 as iio\n",
    "import typing \n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Literal\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For full documentation please refer to https://ai.google.dev/gemini-api/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheme for structured output\n",
    "class TimelineEvent(BaseModel):\n",
    "    \"\"\"Represents a single, classified event in the trial timeline.\"\"\"\n",
    "    time_range: str = Field(description=\"The start and end time of the event in 'start_time - end_time' format, e.g., '0.5s - 1.2s'.\")\n",
    "    \n",
    "    event_classification: str = Field(\n",
    "        description=\"A brief classification of the event (maximum 4 words), e.g., 'Paw reaches for spout' or 'Tongue licks paw'.\"\n",
    "    )\n",
    "    \n",
    "    event_description: str = Field(\n",
    "        description=\"A concise, objective, and detailed description of what was visually observed during the time range.\"\n",
    "    )\n",
    "\n",
    "# Define the main schema for the entire trial analysis\n",
    "class TrialAnalysis(BaseModel):\n",
    "    \"\"\"\n",
    "    Provides a structured analysis of a mouse's behavior in a water-reaching trial.\n",
    "    All fields must be based on clear, indisputable visual evidence only.\n",
    "    \"\"\"\n",
    "    tongue_contact: bool = Field(description=\"Did the mouse's tongue make physical contact with its paw after the paw touched the water drop?\")\n",
    "    water_drop_stable: bool = Field(description=\"Did the water drop remain fully attached to the spout before any interaction?\")\n",
    "    water_spilled: bool = Field(description=\"Did the mouse drop or splash any part of the water drop during retrieval?\")\n",
    "    percentage_consumed: int = Field(description=\"What percentage of the water drop did the mouse drink? Must be 0 if no contact was made.\", ge=0, le=100)\n",
    "    outcome_classification: Literal[\"Successful\", \"Partial Success\", \"Failed\"] = Field(description=\"Classification of the trial's outcome.\")\n",
    "    justification: str = Field(description=\"Brief, evidence-based justification for the outcome classification.\")\n",
    "    timeline: List[TimelineEvent] = Field(description=\"A chronological list of key, observable events.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0\n",
    "top_p = 0.9\n",
    "\n",
    "sys_instruc =(\"You are a research assistant tasked with objectively scoring a mouse's behavior in a water-reaching trial.\" \n",
    "              \"Your analysis must be based only on clear, indisputable visual evidence. Pay extremely close attention to the following elements:\"\n",
    "              \"1. The Water Drop: Note its formation, its position on the spout, and any change to its shape or location.\"\n",
    "              \"2. The Mouse's Tongue (very obvious and similar to that of a dog's): Look for the moment the tongue is extended or if the tongue physically touches the water.\"\n",
    "              \"3. The Mouse's Paws: Observe if the paws lift off the surface and move towards the spout. Mainly the mouse's right paw (the mouse's perspective).\"\n",
    "              \"Most importantly, the mouse is head fixed meaning it cannot move its head and its tongue cannot lick the spout directly.(the mouse's perspective and the one closest to the metal spout when the paws are at rest)\"\n",
    "              \"The water drop can only be consumed by its right paw reaching the water drop on the spout and bringing it close to its mouth for it to lick.\"\n",
    "              \"If the mouse's right paw did not come near to the waterdrop or moved at all, it couldn't have drank any of the water drop.\"\n",
    "              \"The water drop is always delivered within the first 30-40 seconds of the video. In some cases, the water drop might nos stick to the spout for the mouse to touch\"\n",
    "              \"Do not infer success or failure; only report the physical interactions you see. In each video, only one drop is delivered.\"\n",
    "              \"If no change or movement (right paw) is detected for a specific feature (e.g., paw, tongue), you must state 'No change was observed.\"\n",
    "              \"Do not repeat the question in your responses\"\n",
    "              \"Do not describe an event unless there is undeniable visual evidence\")\n",
    "\n",
    "\n",
    "a1 = \"Did the mouse's tongue make physical contact with the water drop? (Answer: Yes or No)\"\n",
    "a2 = \"Did the water drop remain fully attached to the spout before the mouse touched it? (Answer: Yes or No)\"\n",
    "a3 = 'Did the mouse drop or splashed some of the water when retrieving the water to drink ? (Yes or No)'\n",
    "a4 = \"What percentage of the water drop did the mouse drink? (If no physical contact was made between the tongue and the drop, the answer must be 0%).\"\n",
    "a5 = \"\"\"\n",
    "Based on the provided video of a head-fixed mouse performing a water-reaching trial, classify the outcome as one of the following:\n",
    "Successful: The mouse retrieved most of the water (more than 70%).\n",
    "Partial Success: The mouse retrieved some of the water (between 50% and 70%).\n",
    "Failed: The mouse retrieved little or none of the water (less than 40%).\n",
    "Clearly state your classification and briefly justify your choice using only observable evidence from the video.\n",
    "\"\"\"\n",
    "a5 = a5.replace('\\n', ' ')\n",
    "a6 = \"Provide a timeline of key events. Use the following specific format and describe only what you see ( Time start - Time end: Event).\"\n",
    "\n",
    "\n",
    "api_key = '' #add your api key here https://aistudio.google.com/prompts/new_chat\n",
    "MODEL_ID=\"gemini-2.5-pro\" \n",
    "\n",
    "config=types.GenerateContentConfig(\n",
    "    temperature=temperature,\n",
    "    system_instruction = sys_instruc,\n",
    "    top_p=top_p)\n",
    "config = types.GenerateContentConfig(\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    # The structured output settings belong here:\n",
    "    response_mime_type=\"application/json\",\n",
    "    response_schema=TrialAnalysis,\n",
    "    system_instruction = sys_instruc\n",
    ")\n",
    "\n",
    "\n",
    "save_path = '/mnt/team/TM_Lab/Tony/wr_new/data_used/tta_gcamp8s/gemini_predictions_full_sys'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paths_to_dict_unique(file_path):\n",
    "    \"\"\"\n",
    "    Reads a file containing file paths (one per line) and returns a dictionary\n",
    "    with the basename as key and the full path as value. If duplicate basenames\n",
    "    exist, the last one is kept.\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            full_path = line.strip()\n",
    "            if full_path:\n",
    "                base_name = os.path.basename(full_path)\n",
    "                result[base_name] = full_path\n",
    "    return result\n",
    "file_paths = 'vid_paths.txt'\n",
    "path_dict = paths_to_dict_unique(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paths_to_dict_unique(file_path):\n",
    "    \"\"\"\n",
    "    Reads a file containing file paths (one per line) and returns a dictionary\n",
    "    with the basename as key and the full path as value. If duplicate basenames\n",
    "    exist, the last one is kept.\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            full_path = line.strip()\n",
    "            if full_path:\n",
    "                base_name = os.path.basename(full_path)\n",
    "                result[base_name] = full_path\n",
    "    return result\n",
    "file_paths = 'vid_paths.txt'\n",
    "path_dict = paths_to_dict_unique(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing trials in FJ_R3_2024-07-15_1:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing trials in FJ_R3_2024-07-15_1: 100%|██████████| 6/6 [03:53<00:00, 38.96s/it]\n",
      "Processing trials in FJ_L3_2024-07-15_1:  55%|█████▌    | 27/49 [18:04<13:59, 38.14s/it]"
     ]
    }
   ],
   "source": [
    "# Assuming video_paths is a list of video file paths\n",
    "video_paths = [...]  # Your list of video paths here\n",
    "\n",
    "# Create event_log from video paths\n",
    "event_log = pd.DataFrame({\n",
    "    'video_path': video_paths,\n",
    "    'tongue_contact': pd.NA,\n",
    "    'water_drop_stable': pd.NA,\n",
    "    'water_spilled': pd.NA,\n",
    "    'outcome_classification': pd.NA,\n",
    "    'justification': pd.NA,\n",
    "    'raw_answers': pd.NA,\n",
    "    'upload_time': pd.NA,\n",
    "    'api_time': pd.NA,\n",
    "    'full_response': pd.NA\n",
    "})\n",
    "\n",
    "# Find unprocessed videos\n",
    "total = event_log['outcome_classification'].isna().sum()\n",
    "save_name = os.path.join(save_path, \"video_analysis_results.csv\")\n",
    "\n",
    "pbar = tqdm(total=total, desc=\"Processing videos\", leave=True, position=0)\n",
    "\n",
    "try:\n",
    "    for idx, row in event_log.iterrows():\n",
    "        if pd.isna(row['outcome_classification']):\n",
    "            try:\n",
    "                video_path = row['video_path']\n",
    "                \n",
    "                # Upload video directly\n",
    "                start_time = time.time()\n",
    "                video_file = client.files.upload(file=video_path)\n",
    "                \n",
    "                while video_file.state.name == \"PROCESSING\":\n",
    "                    sleep(1)\n",
    "                    video_file = client.files.get(name=video_file.name)\n",
    "                \n",
    "                upload_time = time.time()\n",
    "                \n",
    "                response = client.models.generate_content(\n",
    "                    model=f\"models/{MODEL_ID}\",\n",
    "                    contents=[\n",
    "                        video_file,\n",
    "                        a1,\n",
    "                        a2,\n",
    "                        a3,\n",
    "                        a4,\n",
    "                        a5],\n",
    "                    config=config,\n",
    "                )\n",
    "                \n",
    "                api_call_time = time.time()\n",
    "                client.files.delete(name=video_file.name)\n",
    "                \n",
    "                event_log.loc[idx, 'raw_answers'] = str(response.text)\n",
    "                event_log.loc[idx, 'upload_time'] = upload_time - start_time\n",
    "                event_log.loc[idx, 'api_time'] = api_call_time - upload_time\n",
    "\n",
    "                full_dic = response.to_json_dict()\n",
    "                event_log.loc[idx, 'full_response'] = json.dumps(full_dic)\n",
    "                \n",
    "                analysis: TrialAnalysis = response.parsed\n",
    "                answers = analysis.model_dump_json()\n",
    "                answers = json.loads(answers)\n",
    "                \n",
    "                event_log.loc[idx, 'tongue_contact'] = answers['tongue_contact']\n",
    "                event_log.loc[idx, 'water_drop_stable'] = answers[\"water_drop_stable\"]\n",
    "                event_log.loc[idx, 'water_spilled'] = answers[\"water_spilled\"]\n",
    "                event_log.loc[idx, 'outcome_classification'] = answers['outcome_classification']\n",
    "                event_log.loc[idx, 'justification'] = answers[\"justification\"]\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f'Error processing video at index {idx}')\n",
    "                print(f'Video path: {row[\"video_path\"]}')\n",
    "                print(e)\n",
    "\n",
    "            pbar.update(1)\n",
    "    \n",
    "    event_log.to_csv(save_name, index=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error occurred during processing\")\n",
    "    print(e)\n",
    "    event_log.to_csv(save_name, index=False)\n",
    "\n",
    "pbar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
